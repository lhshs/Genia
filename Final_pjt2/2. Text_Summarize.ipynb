{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Github\\\\Genia\\\\Final_pjt2\\\\Lecture_Text'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory\n",
    "os.chdir('./Lecture_Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NCIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hslio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자막 원본 길이 : 17571\n",
      "RESULT >> 뉴런 수학은 수의 관한 내용과 식에 관한 내용 그리고 함수로 이루어져 있고 이 내용 중에서 처음 공부하는 단원은 유리수와 소수라는 단원이 될 것이며 첫 번째 공부할 내용은 유리수와 소수에 대해 알아볼 것이고 두 번째는 순환 소수에 대해 알아볼 것이다.\n"
     ]
    }
   ],
   "source": [
    "# eenzeenee/t5-base-korean-summarization\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('eenzeenee/t5-base-korean-summarization')\n",
    "tokenizer = AutoTokenizer.from_pretrained('eenzeenee/t5-base-korean-summarization')\n",
    "\n",
    "# Specify the file path\n",
    "file_path = './txt/01강유리수의소수표현(1)_EBS중학뉴런수학2(상).txt'\n",
    "\n",
    "# Read the text from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "print(f'자막 원본 길이 : {len(text)}')\n",
    "\n",
    "# inputs = [prefix + sample]\n",
    "inputs = [text]\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=3, do_sample=True, min_length=1000, max_length=10000) \n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "result = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "print('RESULT >>', result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate For Summarize\n",
    "- Kor -> Eng -> Summarize -> Kor\n",
    "- Translate : https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt \n",
    "- Summarize : https://huggingface.co/facebook/bart-large-cnn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자막 원본 길이 : 17571\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path\n",
    "file_path = './txt/01강유리수의소수표현(1)_EBS중학뉴런수학2(상).txt'\n",
    "\n",
    "# Read the text from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(f'자막 원본 길이 : {len(text)}')\n",
    "\n",
    "# Define a function to split the text into chunks\n",
    "def chunk_text(text, chunk_size):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def google_trans_to_ko(messages):\n",
    "    google = Translator()\n",
    "    result = google.translate(messages, dest=\"ko\")\n",
    "    return result.text\n",
    "\n",
    "def google_trans_to_en(messages):\n",
    "    google = Translator()\n",
    "    result = google.translate(messages, dest=\"en\")\n",
    "    return result.text\n",
    "\n",
    "# Specify the chunk size\n",
    "chunk_size = 500  # Adjust this value based on your needs\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = chunk_text(text, chunk_size)\n",
    "\n",
    "# Initialize the final result\n",
    "final_result = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "facebook/mbart-large-50-many-to-many-mmt -> facebook/bart-large-cnn -> facebook/mbart-large-50-many-to-many-mmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but your input_length is only 136. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n",
      "Your max_length is set to 142, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샌드맨은 수학 수학 신경학의 2학년입니다. 우리가 2학년에서 할 일 중 가장 중요한 것은 숫자, 음식, 그리고 기능을 배울 것입니다. 우리 강의의 제목은 수학: 뉴런이란 무엇일까요? 반지란 무엇일까요? 우리는 달리고 배우고 있습니다.유리 숫자는 분자의 형태로 표현할 수 있는 숫자입니다. 우리가 3학년 때부터 좀 더 정교하게 얘기할 것입니다. 우리는 유리 숫자와 소수에 대해 배웠습니다. 그리고 두번째로, 새로운 것이 생겨나고 있습니다.b는 0이 필요하지 않습니다. 그래서 우리는 이 숫자를 유리 숫자로 표현할 수 있는 숫자라고 부릅니다. 그래서 지금까지 우리가 알고 있는 거의 모든 숫자들은 그래프로 표현할 수 있습니다. 하지만 그 중 하나를 표현하기가 어려운 경우, 첫번째 반에 배운 π가 그 숫자일까요?우리는 숫자를 '정한'이라고 부릅니다. 그리고 영원히 지속되는 숫자가 있습니다. 우리는 이 무한한 숫자를 '영원히'라고 부릅니다. 그리고 여러분의 조상들이 여러분에게 물었을 때, \"교수님, 왜 이런가요? 0이 아니라 무한한 숫자입니다. 왜 이렇게 부릅니다?\"\"이것이 영원히 지속된다면, 이것은 무한하다.\" 여러분은 그렇게 할 수 있습니다. 그래서 우리는 \"아니요, 무한하다\"라고 말하지 않는 이유입니다. 무수보다 아래에 무한한 숫자가 있다는 의미입니다. 그래서 \"이것이 무한하지 않나요?\"라고 생각하기 쉽습니다. 왜냐하면 그 중간에 0이 있기 때문이죠.그 뒤에는 아무것도 없죠. 그리고 여러분은 \"엄마, 0.3333 다음에는 세 개가 더 있을거야.\" 라고 생각하죠. 만약 더 많은 것을 원한다면, 작은 점을 추가해야만 합니다. 하지만 아니죠. 그 작은 점은 소수 아래 네번째 자리에 끝납니다. 그래서 무한의 숫자입니다.\"주기적인 숫자란 무엇일까요? 그게 제가 2학년 때 만들어낸 첫 번째 용어입니다.\"라고 그는 말합니다. 지구상의 공기는, 우리 나라의 꼭대기에 매일 우리 나라 위에 있는 것이 아닙니다. 낮은 압력, 높은 압력, 북태평양, 시베리아의 낮은 압력 등등이 있습니다. 그래서 공기는 계속 돌아오죠.소수 밑에 있는 특정한 점은 스스로 반복됩니다. 그리고 소수는 기본적으로 반복되면 영원히 지속됩니다. 그럼 이게 무엇일까요? 3와 5는 영원히 지속됩니다? 0.6123123123, 그 다음에 123가 계속된다고 생각할 수 있습니다.밑에 있는 숫자는 숫자의 순환입니다. 선생님은 \"수들의 순환\"의 점을 강조합니다. 밑에 있는 숫자는 또한 \"수들의 순환\"입니다. 선생님은 말합니다. \"순환\"은 숫자가 위에 있는 숫자의 밑과 위에 있는 순환의 점을 의미합니다.\"와\"는 여러분의 세계에 대한 지식의 시험입니다. \"와\"는 영어로 \"지르는\" 또는 \"지르는\"을 의미합니다. \"와\"라는 단어는 사람이나 사건을 포함해서 여러가지를 의미하는데 사용됩니다. \"와\"라는 단어는 \"와\"라는 글자로 시작하고 \"와\"라는 단어로 끝나는 단어의 연속입니다.\"목표가 어디에서 중요한지 아시죠? 이제 다음으로 넘어가겠습니다.\" 그는 말합니다. \"다음으로, 물론, 주기적인 숫자의 표현이 될겁니다. 왜 이게 나오죠?\"한 점은 끝에 있는 점이고, 원이 다른 끝에 있는 점이고, 원의 꼭대기에 있는 점이고, 원의 중간에 있는 점이고, 원이 한 끝에 있는 점이 다른 끝에 있는 점이죠.\"우리는 약속을 필요로 했고, 양쪽 끝에 점 하나로 표현했습니다. 만약 여러분이 이 모든 것을 기억할 수 있다면, 이걸 확인해달라고 부탁드리겠습니다.\" 그는 말합니다. \"그래서 85을 써야 해요.\" 그는 더합니다. \"이걸 계속 반복해서 해야 해요.\"CNN.com의 존 서터는 순수 문장에서 주기적인 문장을 어떻게 표현해야 하는지에 대한 문제를 다루었습니다. 서터: \"진수에 따라 특정한 기간이 반복되고 가장 작은 단위인 0.555... 물론 5은 주기적인 문장이라고 말할 수 있습니다.\"1.32 132 132... 이렇게 읽으면 132라고 생각하기 쉽습니다. 하지만 이 기간은 소수 밑에서 반복되는 기간이기 때문에 132 대신 321을 써야 합니다. 그리고 4번째는 0.1525252... 무슨 일이 일어날까요? 52 52 52는 루프입니다. 그렇기 때문에 첫번째 단어 52를 써야 합니다. 여러분은 정확한 표현이 4이라는 것을 보실 겁니다.3점만 있으면 괜찮아요. 왜냐구요? 왜냐하면 원 끝에 2점만 있으면 5점을 볼 필요가 없으니까요. 제대로 고쳐보면 0.303030... 30인가요? 원 끝에 1점만 있으면 볼 수 있죠. 하지만 속도를 늦추겠습니다.0.52를 써서 원에 가리키면, 맨 위의 두 개를 가리키는 것이 옳습니다. 그리고 그것이 세번째입니다. 네 번 가리키는 것은 옳지 않습니다. 2.4555로 가보죠. 원의 첫번째 다섯 개로요. 그렇게 표현해야 합니다.주기적인 숫자의 표현은 주기적인 숫자의 첫 글자를 0보다 아래로 써서 양쪽 끝을 가리키는 것입니다. 우리는 5분의 11분을 소수로 표현할 것입니다. 소수로 표현하는 방법은, 어떻게 분할할까요?5를 말하는 순간은 정말 감각이 됩니다. 이런 식으로 진행하는 것이 의미하는 바는, 소수를 내려다 보세요. 첫번째 소수, 세번째, 다섯번째, 단위 소수는 4입니다. 두번째는 소수 밑에요? 네번째? 여섯번째?다음의 네번째 예는 숫자의 일부분을 나타냅니다. 다음 시간과 직접 연결될 것입니다. 초등학교에서 우리는 이것을 배웠고, 이것을 함께 살펴볼 것입니다. 여러분이 이것을 이해할 수 있다면, 어떻게 해결해야 하는지 아실 겁니다. 이것의 적용을 보면, 우리는 훨씬 더 어려운 문제를 보게 될 것입니다.만일 우리가 소수로 제한된 숫자를 정한다면, 그것은 10, 100, 또는 1,000입니다. 제가 말하고자 하는 것은, 이걸 계산해보자는 것입니다. 67의 10,000입니다. 말하기 어렵지 않습니다. 왜냐구요? 우리가 나누는 아이디어죠.우리는 숫자를 1으로 곱하면 바꿀 수 없습니다. 그렇죠? 하지만 1은 분자의 숫자와 동일한 숫자입니다. 이 개념을 들어보세요. 우리는 이 숫자의 가치의 크기를 바꿀 수는 없습니다. 하지만 그냥 숫자를 100으로 쓰고 싶다면 그것을 곱할 수 있습니다. 그렇죠.이 수분법의 형태에서 우리가 신경써야 할 것은, 만일 10이 2x5이고 100이 22x52이고 1000이 23x53라고 하면, 우리는 2와 5의 수분법으로 끝납니다. 그리고 바닥은 2와 50,000입니다.2는 1x1의 숫자이고, 50만은 2입니다. 그리고 2와 5와 같은 숫자로 만들어야 합니다. 자, 50을 2로 곱하면 2x5의 곱셈입니다. 10번째 곱셈이 아닙니다.만약 밑에 문제가 있다면, 우리는 문제를 해결하는 연습을 시작할 것입니다. 여러분이 문장에 얻으려고 하는 a+b+c의 가치는, 만약 a가 2이고 b가 18이고 c가 0.18이라면 20입니다. 자, 이것이 바로 우리가 다음 강의에서 이야기 하고자 하는 것입니다.CNN.com의 존 서터는 학생들에게 뉴런 교과서에 있는 모든 문제를 해결해달라고 요구합니다. 서터: \"9세기의 숫자를 네 개로 표현할 때 주기적인 숫자는 무엇일까요?\" 서터는 답이 0인 4에서 9이라고 합니다.CNN.com의 John Defterios는 CNN.com 역사에서 가장 기억할 수 있는 순간들을 되돌아봅니다. 우리는 다음 주에 \"This Is CNN\"라는 새로운 시리즈에 등장하는 CNN.com의 가장 기억할 수 있는 에피소드 중 하나를 되돌아보겠습니다.CNN의 존 서터는 루비크의 큐브에 대한 답을 찾는 방법을 보여줍니다. 약간 어렵지만, 스스로 해내는 습관이 있어야 합니다. 서터는 이렇게 말합니다. 8개가 될겁니다. 4 x 8은 32이고 352입니다.만약 27분의 4, 4 ÷ 27을 하면 0.148148가 나옵니다. 이렇게 보입니다. 자, 소수 아래의 첫번째 숫자에서 20번째 숫자로 무엇을 얻을 수 있을까요? 제가 말했죠, 합을 얻으세요. 여기서부터 계속 이것을 추가하십니까?결과는 3개의 숫자가 있는 括弧 안에 있는 숫자와 같습니다. 만일 3개의 숫자를 합친다면, \"3 x 6 = 18\"라고 말합니다. 18번째는 1+4+8로 끝납니다. 두 개는 같은 것입니다.여러분은 이것을 알아야 합니다. 이것은 중요한 문제입니다. 이것은 시험에서 자주 발생하는 문제입니다. 그리고 여러분은 이것을 잘못 이해하는 문제입니다. 다음 패턴은 이것을 25 대 6의 곱하기로 제한적인 숫자로 표현하는 것입니다. 자 그럼 a, b, c 값을 찾아봅시다.분자는 6 x 4입니다. 24는 100 x 24입니다. 52, 22는 5 x 2 곱하기 102입니다. 그래서 우리는 그것을 유한한 숫자로 쉽게 표현할 수 있습니다. 그래서 22의 가치는 무엇일까요? 22는 4일겁니다. 24는 0.24입니다. 그래서 결국 102는 100입니다.다음은 40분짜리 숫자의 7을 한정된 숫자로 표현하는 과정입니다. 각각에 맞는 a, b, c 값을 얻습니다. 아래쪽은 조금 더 복잡하고, 위쪽은 조금 더 쉽습니다. 그래서 이 문제를 해결할 수 있다는 것을 기억하세요.\"당신이 함께 신경 수학을 공부하는 것을 재미있게 해주시길 바랍니다.\" 라고 그는 말합니다. \"선생님 얼굴이 떠오르면, 제가 여러분을 자주 보았을 때, 그 옆에 있는 남자나 삼촌처럼, 아주 적응력이 있을 거라고 생각합니다.\" 그는 더합니다. \"그냥 끝낼 수는 없어요. 제가 여러분과 함께 해보겠습니다.\"우리는 안경의 숫자의 표현을 살펴봤습니다. 그리고 분자 분산을 통해 그것을 분자에 표현하는 방법을 알아냈습니다. 하지만 모든 무한한 숫자들 중에서 오늘날 우리가 배운 것은 무엇일까요? 우리는 그것이 순환수라는 것을 배웠습니다. 그 숫자 밑에 있는 것이 오늘날의 열쇠입니다.\"이걸 미리 읽어서 강연이 끝날 때 저를 만날 수 있다면 도움이 될 것 같네요.\" 라고 그는 말합니다. \"저가 작업한 것에 대한 약간의概要을 보여드리겠습니다.\" 라고 덧붙입니다. \"그건 여러분이 지난 몇 주 동안 보신 것과 약간 다르게 될 것입니다.\"\n",
      "최종 요약 길이 : 4816\n"
     ]
    }
   ],
   "source": [
    "# Process each chunk\n",
    "for chunk in chunks:\n",
    "    #####\n",
    "    # Translate Korean to English\n",
    "    input = chunk\n",
    "\n",
    "    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "    \n",
    "    tokenizer.src_lang = \"ko_KR\"\n",
    "    encoded_hi = tokenizer(input, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded_hi,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "    )\n",
    "    to_eng = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    #####\n",
    "    # Summarize English\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    ARTICLE = to_eng[0]\n",
    "    eng_sum = summarizer(ARTICLE, do_sample=False)\n",
    "    \n",
    "    #####\n",
    "    # Translate English to Korean\n",
    "    input = eng_sum[0]['summary_text']\n",
    "    \n",
    "    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "    \n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    encoded_hi = tokenizer(input, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded_hi,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"ko_KR\"]\n",
    "    )\n",
    "    result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Append the result to the final result\n",
    "    final_result += result[0]\n",
    "\n",
    "print(final_result)\n",
    "print(f'최종 요약 길이 : {len(final_result)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google_trans -> facebook/bart-large-cnn -> google_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자막 원본 길이 : 17571\n"
     ]
    }
   ],
   "source": [
    "# Define a function to split the text into chunks\n",
    "def chunk_text(text, chunk_size):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def google_trans_to_ko(messages):\n",
    "    google = Translator()\n",
    "    result = google.translate(messages, dest=\"ko\")\n",
    "    return result.text\n",
    "\n",
    "def google_trans_to_en(messages):\n",
    "    google = Translator()\n",
    "    result = google.translate(messages, dest=\"en\")\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자막 원본 길이 : 805141\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path\n",
    "file_path = './중학 뉴런 수학 2학년 (상) 손석민_all.txt'\n",
    "\n",
    "# Read the text from the file\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Specify the chunk size\n",
    "chunk_size = 400  # Adjust this value based on your needs\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = chunk_text(text, chunk_size)\n",
    "\n",
    "# Initialize the final result\n",
    "final_result = ''\n",
    "\n",
    "print(f'자막 원본 길이 : {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 :반갑습니다, EBS 친구 여러분들. 중학 뉴런 수학 2학년 (상)의 손석민입니다. 여러분들과 함께 새로운 강좌로 만나게 되었습니다. 일단 반갑다는 인사를 드리고 싶고요.\n",
      "우리 교재 제목, 뉴런 수학이죠? ‘런’이 뭐예요? 달리기도 있고 배우는 것도 있어요. ‘뉴’는 뭐예요? ‘새로운’입니다.\n",
      "우리가 수학을 새롭게 공부해 볼 거고, 새롭게 달린다는 마음으로, 2학년이 되었으니까 그렇게 공부를 했으면 하는 선생님의 바람입니다.\n",
      "우리가 2학년 (상)에서 주로 공부하게 될 내용은 바로 수의 관한 내용, 식에 관한 내용, 함수 이렇게 이루어져 있는데요. 이 내용 중에서 우리 그럼 처음 공부하는 단원은 바로 ‘유리수와 소수’라는 단원이 되겠고요.\n",
      "그래서 우리 첫 번째 공부할 주제를 잡아 보면 바로 ‘유리수의 소수’ 편\n",
      "영어 변환 :Nice to meet you, EBS friends. This is Son Seok-min, a second-year middle school student in Neuron Mathematics. We have a new course with you all. First of all, I would like to say hello to you.\n",
      "The title of our textbook is Neuron Mathematics, right? What is ‘Run’? There is running and there is learning. What is ‘new’? It is ‘new’.\n",
      "The teacher's hope is that we will study mathematics in a new way, and that we will study like that since we are in second grade with the mindset of having a fresh start.\n",
      "The content we will mainly study in the second year (upper) consists of numbers, expressions, and functions. Among these contents, the first unit we study will be ‘Rational Numbers and Prime Numbers.’\n",
      "So, if we choose our first topic to study, it is ‘Prime Numbers of Rational Numbers’.\n",
      "영어 요약 :Nice to meet you, EBS friends. This is Son Seok-min, a second-year middle school student in Neuron Mathematics. We have a new course with you all. First of all, I would like to say hello to you.\n",
      "The title of our textbook is Neuron Mathematics, right? What is ‘Run’? There is running and there is learning. What is ‘new’? It is ‘new’.\n",
      "The teacher's hope is that we will study mathematics in a new way, and that we will study like that since we are in second grade with the mindset of having a fresh start.\n",
      "The content we will mainly study in the second year (upper) consists of numbers, expressions, and functions. Among these contents, the first unit we study will be ‘Rational Numbers and Prime Numbers.’\n",
      "So, if we choose our first topic to study, it is ‘Prime Numbers of Rational Numbers’.\n",
      "한글 결과 :손석민은 뉴런수학과 중학교 2학년이다. 2학년 때 주로 공부할 내용은 숫자, 표현, 함수로 구성됩니다. 이 내용 중 첫 번째로 공부할 단원은 '유리수와 소수'입니다.\n",
      "<--------------------------------------->\n",
      "원본 데이터 :입니다. 그래서 첫 번째 공부할 내용 ‘유리수와 소수’에 대해 알아볼 거고, 두 번째로는 순환 소수라는 거에 대해서 알아보도록 할 거예요.\n",
      "여기서 여러분들 유리수와 소수 첫 번째 공부할 내용은 이미 들어봐서 알고 있어요. 유리수 배웠고, 소수라는 것도 ‘0.××’ 하는 소수를 얘기하는데, 이미 알고 있습니다.\n",
      "그럼 두 번째, 새롭게 나온 게 있죠. 바로 순환소수라는 것이 무엇인지 이 부분에 신경 써서 오늘 수업을 함께 하면 되겠습니다. 같이 개념에서부터 한 번 알아보도록 하겠습니다.\n",
      "핵심 개념 콕콕입니다. 우리 첫 번째, 유리수와 소수에 대해서 알아본다고 했는데요. 먼저 유리수의 뜻. 우리는 중학교 1학년 과정에서도 공부했지만, 유리수라는 건 분수의 꼴로 나타낼 수 있는 수를 이야기합니다.\n",
      "그런데 우리가 중\n",
      "영어 변환 :no see. So, the first thing we will study is ‘rational numbers and decimals’, and the second thing we will learn is circular decimals.\n",
      "Here, you have already heard and know the first part of your study: rational numbers and decimals. I learned about rational numbers, and I already know that prime numbers are decimals like ‘0.××’.\n",
      "Then, secondly, there is something new. Let's take today's class by paying attention to what a circular decimal is. Let’s take a look at the concept together.\n",
      "This is the key concept. First, we said we would learn about rational numbers and decimals. First, the meaning of rational numbers. We studied it in the first year of middle school, but rational numbers are numbers that can be expressed as fractions.\n",
      "But we are\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Github\\Genia\\Final_pjt2\\2. Text_Summarize.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Genia/Final_pjt2/2.%20Text_Summarize.ipynb#Y103sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m summarizer \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39msummarization\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfacebook/bart-large-cnn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Genia/Final_pjt2/2.%20Text_Summarize.ipynb#Y103sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ARTICLE \u001b[39m=\u001b[39m to_eng\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Github/Genia/Final_pjt2/2.%20Text_Summarize.ipynb#Y103sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m eng_sum \u001b[39m=\u001b[39m summarizer(ARTICLE, do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Genia/Final_pjt2/2.%20Text_Summarize.ipynb#Y103sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m영어 요약 :\u001b[39m\u001b[39m{\u001b[39;00mto_eng\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Genia/Final_pjt2/2.%20Text_Summarize.ipynb#Y103sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#####\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Genia/Final_pjt2/2.%20Text_Summarize.ipynb#Y103sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Translate English to Korean\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:269\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    246\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    169\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[0;32m    170\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[0;32m    171\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[0;32m    172\u001b[0m     ):\n\u001b[0;32m    173\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\pipelines\\base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\pipelines\\base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:191\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m     in_b, input_length \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(model_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(\n\u001b[0;32m    187\u001b[0m     input_length,\n\u001b[0;32m    188\u001b[0m     generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmin_length),\n\u001b[0;32m    189\u001b[0m     generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length),\n\u001b[0;32m    190\u001b[0m )\n\u001b[1;32m--> 191\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    192\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\generation\\utils.py:1752\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1746\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1747\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1748\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1749\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   1753\u001b[0m         input_ids,\n\u001b[0;32m   1754\u001b[0m         beam_scorer,\n\u001b[0;32m   1755\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1756\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1757\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1758\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1759\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1760\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1761\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1762\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1763\u001b[0m     )\n\u001b[0;32m   1765\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[0;32m   1766\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\generation\\utils.py:3091\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3087\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   3089\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3091\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   3092\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   3093\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   3094\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   3095\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   3096\u001b[0m )\n\u001b[0;32m   3098\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3099\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1577\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1573\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1574\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1575\u001b[0m         )\n\u001b[1;32m-> 1577\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1578\u001b[0m     input_ids,\n\u001b[0;32m   1579\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1580\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1581\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[0;32m   1582\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1583\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1584\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1585\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1586\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1587\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1588\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1589\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1590\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1591\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1592\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1593\u001b[0m )\n\u001b[0;32m   1595\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\n\u001b[0;32m   1596\u001b[0m lm_logits \u001b[39m=\u001b[39m lm_logits \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\u001b[39m.\u001b[39mto(lm_logits\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1463\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1457\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m   1458\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1459\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1460\u001b[0m     )\n\u001b[0;32m   1462\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1463\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1464\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1465\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1466\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m   1467\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1468\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1469\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1470\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1471\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1472\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1473\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1474\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1475\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1476\u001b[0m )\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1479\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1316\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1304\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m   1305\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         use_cache,\n\u001b[0;32m   1314\u001b[0m     )\n\u001b[0;32m   1315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1316\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m   1317\u001b[0m         hidden_states,\n\u001b[0;32m   1318\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1319\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1320\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1321\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1322\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[0;32m   1323\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m   1324\u001b[0m         ),\n\u001b[0;32m   1325\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1326\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1327\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1328\u001b[0m     )\n\u001b[0;32m   1329\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\anaconda3\\envs\\lhs_3.9\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:672\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[0;32m    671\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m--> 672\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(hidden_states))\n\u001b[0;32m    673\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m    674\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(hidden_states)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process each chunk\n",
    "for chunk in chunks:\n",
    "    #####\n",
    "    # Translate Korean to English\n",
    "    input = chunk\n",
    "    print(f'원본 데이터 :{input}')\n",
    "    to_eng = google_trans_to_en(input)\n",
    "    print(f'영어 변환 :{to_eng}')\n",
    "\n",
    "    #####\n",
    "    # Summarize English\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    ARTICLE = to_eng\n",
    "    eng_sum = summarizer(ARTICLE, do_sample=False)\n",
    "    print(f'영어 요약 :{to_eng}')\n",
    "    \n",
    "    #####\n",
    "    # Translate English to Korean\n",
    "    input = eng_sum[0]['summary_text']\n",
    "\n",
    "    result = google_trans_to_ko(input)\n",
    "    print(f'한글 결과 :{result}')\n",
    "\n",
    "    # Append the result to the final result\n",
    "    final_result += result\n",
    "    print('<--------------------------------------->')\n",
    "\n",
    "print(final_result)\n",
    "print(f'최종 요약 길이 : {len(final_result)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"손석민은 뉴런수학과 중학교 2학년이다. 2학년 때 주로 공부할 내용은 숫자, 표현, 함수로 구성됩니다. 이 내용 중 첫 번째로 공부할 단원은 '유리수와 소수'입니다.유리수는 분수로 표현할 수 있는 숫자입니다. 원형 소수는 원형 모양의 숫자입니다. 유리수와 소수를 먼저 공부하겠습니다. 그런 다음 유리수를 다루기 위해 순환소수를 사용하는 방법을 배우겠습니다. 안돼요. 아니 알겠습니다.예를 들어 5/3, 3, 5는 정수이고, 마이너스 7/5, 5, -7은 이렇게 생각할 수 있습니다. 3이 되면 상황이 조금 더 복잡해지기 때문에 지금 우리가 알고 있는 분수는 분수이고, 이렇게 이야기하면 b와 a는 모두 정수형입니다.유한십진수는 극한이 있는 십진수입니다. 우리는 이러한 소수를 유한소수라고 부르는데, 끝없이 이어지는 소수도 있습니다. 첫 번째는 유한소수이고, 두 번째는 무한소수이다. 부모의 사랑에는 한계가 없습니다. 무엇? 끝이 없단 말이에요.즉, 갈 수 있는 곳은 끝이 없습니다. 계속해서 이어지는 이러한 소수를 무한소수라고 합니다. 예를 들어 0.333… 이것은 무엇을 의미하나요? 이는 숫자 3이 끝없이 계속된다는 뜻이다. 그리고 나는 끝없이 나아갈 것이다. 6.252525… 그리고 끝없이 이어지는데,교사: 0.2는 정말 유한십진수인가요? 나는 그렇게 생각하지 않습니다. 무한대로 가면 무한한 십진수이다. 그럼 0이 아닌 숫자만 있어야 한다고 생각하시나요? 그렇다면 0.205205… 이것은 무한소수가 아닌가? 그냥 0이 이렇게 끝없이 계속되는 뿐입니다.소수점 이하 네 번째 자리에서 끝나므로 유한소수이다. 무한소수는 어차피 끝없이 이어지기 때문에 보통 딩딩딩딩처럼 뒤에 붙는다고 생각하면 된다. 아주 쉽게 확인하실 수 있습니다. 뒤에는 아무것도 없습니다. 0.454545.우리는 유리수가 무엇인지 배웠고 소수는 유한과 무한으로 나뉩니다. 우리는 실제 소수를 살펴보고 유한과 무한을 구별했습니다. 이제 우리가 공부할 다음 것은 Decimals를 반복하는 것입니다. 2학년 입학 이후 처음으로 나오는 용어다. 순환이란 무엇입니까?우리나라 위의 공기는 매일 우리 위에만 있는 것이 아닙니다. 저기압, 고기압, 북태평양과 시베리아 기단이 있기 때문에 공기는 지구 주위를 계속 순환하게 됩니다. 순환소수는 소수점 아래에 특정 구간이 반복되는 것을 의미합니다.무한한 십진수입니다. 무한소수 중에는 순환소수가 있습니다. 123이 계속 반복되고 끝없이 이어진다고 생각하실 수도 있습니다. 우리는 이러한 소수자를 무엇이라고 부를까요? 반복소수라고 합니다. 반복 가능한 소수점은 어디에 있나요? InfiniteDecimal 중 순환소수입니다.순환소수점은 바로 여기에 있습니다. 특정 구간이 소수점 위가 아닌 소수점 이하에서 반복되는 구간을 순환노드라고 부릅니다. 예를 들어 가장 짧은 부분은 다음과 같습니다. 0.353535... 35가 유통 중입니다. 2727이 반복되므로 순환소수는 '27'입니다.4.27427427, '427은 원형 노드입니다' 답을 쓰세요. 그래서 시험 볼 때 댕! 그리고 당신은 틀렸어요. 그거 찾았 어? 예. 선생님은 이것이 루프의 중요한 부분이라고 말씀하셨습니다. 소수점은 어디에 있나요? 아래에서 왔습니다.이렇게 쓰면 왠지 274라는 숫자가 좀 더 잘 보이는 것 같은 느낌이 듭니다. 그런데 위와 같이 쓰면 427427427 묶음으로 나오므로 427이라고 생각하기 쉽습니다. 실제로는 순환노드란 아래에 반복되는 특정 구간이라는 점을 명심해야 합니다. 소수점.그래서 우리가 여기서 공부할 것은 그 약속이 어떻게 이루어졌는지 알아내는 것입니다. 순환소수는 어떻게 표현하나요? 첫 번째 반복되는 숫자까지만 씁니다. 적어놓은 뒤 원형마디 양쪽 끝에 점을 찍었는데 어차피 원형마디는 한자리 숫자입니다.원형 노드가 123개이므로 점은 전체가 아닌 양쪽 끝에만 배치됩니다. 쓰고 나면 원형의 마디가 3과 5가 됩니다. 그러니 양쪽 끝에 점을 찍기만 하면 됩니다. 이해하시죠? 그렇다면 순환소수를 표현하는 방법으로 점은 왜 등장했을까요? 몇 자릿수일까요?나는 다음 순환소수의 순환노드를 찾아달라는 요청을 받았습니다. 당신이 해야 할 일은 가장 작은 단어, 즉 소수점 이하의 특정 부분을 반복하는 단어를 읽는 것뿐입니다. 숫자가 반복되는지 확인하기 위해 숫자를 반복해서 써야 하는지 여부에 대한 기준은 사람마다 다르기 때문에 약속이 필요합니다.특정 구간이 소수점 이하에서 반복되는데, 이를 가장 작은 단위라고 합니다. 따라서 원형 노드는 231입니다. 이것이 답을 찾는 방법입니다. 어때요? 이렇게 행복하게 웃으며 시작할 수 있겠죠? 하지만 해결하기 다소 어려운 문제도 있을 것입니다. 우리의 대표적인 문제부터 함께 시작해볼까요?가장 작은 노드를 찾고 있으니 숫자 5를 보면 됩니다. 555가 순환한다고 할 수 있지만 실제로는 5입니다. 따라서 30이 순환됩니다. 03이 아닙니다. 따라서 세 번째인 30을 써야 합니다. 이것은 동일합니다. 1.32 132 132… 이렇게 읽으면 132라고 생각하기 쉽습니다. 그런데 특정 구간이 소수점 이하에서 반복되기 때문에,. 132 대신 321을 써야겠죠? 그다음 네 번째, 0.1525252… 무슨 일이야?첫 번째 문제는 소수점 아래에 소수를 반복하는 문제입니다. ④가 올바른 표현임을 알 수 있습니다. 이제 두 번째 문제의 문제점이 무엇인지 알아보겠습니다. 답은 세 개의 점을 찾으면 찾을 수 있는 원형 노드입니다.원형 노드의 양쪽 끝에 점이 배치됩니다. 숫자 1 위에 점이 있으면 보지 않아도 틀린 것입니다. 소수점을 보면 142 142가 반복됩니다. 그래서 우리는 2.142와 같이 쓰지 않습니다.0.253은 253이므로 253을 3개 사용하는 것이 좋았습니다. 그런데 무슨 문제가 있나요? 이렇게 세 번째 자리까지 쓰는 게 아니라 2.45까지 쓰고 원형 마디에 점을 찍어서 이렇게 표현해야 합니다.반복소수를 표현할 때에는 소수점 이하 반복절점의 첫 자리만 쓴다. 이렇게 포인트를 얻을 수 있습니다. 그래서 2번과 3번에만 점을 찍어야 하므로 2번에만 adot을 넣어야 하는데 원형 노드는 모두 점이 있기 때문에 올바르지 않습니다.분수를 소수로 표현하는 방법은 무엇으로 나누는가? 분자를 분모로 나눕니다. 'I'm in'이라고 쓰는 순간 느낌이 옵니다. 0.454545… 그렇죠. 즉, 첫 번째 루프를 작성한 후 양쪽 끝에 점이 추가됩니다.짝수는 5로 끝납니다. 따라서 소수점 둘째, 넷째, 여섯째 자리까지 가면 30번째도 짝수입니다. 즉, 당신이 해야 할 일은 답을 찾는 것뿐입니다. 이와 관련된 적용 문제를 살펴보면 조금 더 어려운 문제들이 많이 보입니다.9/50이라는 분수를 유한소수로 표현하는 과정입니다. 우리는 이것을 초등학교에서 배웠습니다. 분수를 소수로 바꾸고 싶다면 분모를 10, 100, 1000으로 바꿔주면 됩니다. 0.3은 무엇이었나요? 3/10이에요. 0.27은 무엇입니까? 27/100입니다.3/10, 27/100, 781/1000 등 분모가 10, 100, 1000이면 숫자를 십진수로 변환하기 쉽고, 그 다음에는 소수가 유한소수가 됩니다. 67/10000. 이게 어려운가요? 67¼10000 직접 하시나요? 나는 그것을하지 않습니다. 왜?50을 100으로 만들려면 분모를 두 배로 늘리면 됩니다. 숫자가 변경되는 것을 원하지 않으면 숫자에 1을 곱하면 변경되지 않습니다. 단, 1은 분자와 분모가 같은 수라는 뜻입니다. 개념을 잘 들어야 합니다. 그래서 저는 이 숫자의 크기를 바꾸지 않을 것입니다.분모를 두 배로 늘려 100이 되고, 분자를 두 배로 늘려 18이 되는데, 이는 0.18과 같은 소수로 빠르게 변경할 수 있다는 뜻입니다. 9¶50을 하지 않더라도. 결국 c에 대한 답은 0.18로 쓸 수 있다.결국 소인수를 2와 5의 곱으로 분해했습니다. 즉, 지수가 동일하면 10 전체의 세제곱, 즉 2×5로 쓸 수 있다는 뜻입니다. 그렇다면 이것도 만들겠다고 하더군요. 2의 지수는 이제 1이고, 50,000의 지수는 2입니다.우리는 high 5의 지수를 동일하게 만들어야 합니다. 그렇게 하려면 분모에 2를 곱하여 2의 제곱을 구하세요. 따라서 우리는 진짜입니다. 100이고, 분자도 3²×2이므로 18입니다. 이렇게 구할 수 있다는 뜻입니다.본 내용은 다음 강의와 바로 연결되므로 정확하게 기억하셔야 합니다. 따라서 찾고자 하는 a+b+c의 값은 20.18입니다. 왜냐하면 a는 2, b는 18, c는 0.18이기 때문입니다. 즉, 답에 도달할 수 있어야 한다는 뜻입니다.오일 테스트의 경우 선생님이 모든 문제를 해결하지 못하는 경우 태블릿 솔루션을 태블릿으로 제공해드립니다. 틀린 문제는 꼭 확인해주세요. 풀을 풀고 함께 듣기만 하면 됩니다.에멀젼 4번의 경우는 이렇습니다. 3/44를 반복소수로 표현했습니다. 그러면 모양은 똑같습니다. 어떻게 해야 하나요? 우리는 분자를 분모로 나누는 작업을 할 것입니다. 마지막으로 3¼44를 시도해 보겠습니다. 그러면 0이 들어갈 것이다.사실 여기 선택 항목만 보면 소수점 둘째 자리가 6입니다. 이제 6×4=24입니다. 이렇게 들어갑니다. 그러면 36개가 남습니다. 그럼 여기 비용은 얼마나 될까요? 8.8 물론 옵션을 보면 무엇을 얻을지 추측할 수 있습니다.따라서 첫 번째 원형 측정까지 첫 번째 숫자를 쓰면 0.0681이 됩니다. 순환수는 81이므로 두 수의 양쪽 끝에 점을 찍을 수 있다는 점만 찾으면 됩니다. 결국, 당신은 몇 번이나 선택할 수 있습니까? 그게 5번이에요.4/27, 4¼27을 하면 0.148148이 나오는데... 이렇게 나옵니다. 분자를 분모로 나눕니다. 결국, 반복되는 숫자를 이용하여 반복소수로 표현하면 148 148이 됩니다.결국 결과는 괄호 안의 숫자, 즉 세 숫자의 집합과 같습니다. 즉 6번 나오면 6번이 추가된다는 뜻이다. 이렇게 계속 더하면 언젠가는 1+4+8이 되고, 이 마지막 8은 소수점 몇 자리가 될 것이다.3개로 그룹화되어 있으므로 3×6=18입니다. 18번째 숫자는 8로 끝납니다. 그렇다면 19번째 숫자는 무엇일까요? 또 1이군요. 20번째 숫자는 무엇인가요? 4가 됩니다. 이렇게 답을 찾으면 됩니다.2와 5의 곱을 추측하는 것이었습니다. 시험에 자주 나오는 문제이고, 많이 틀리는 문제입니다. 이제 다음 문제로 넘어 갑시다. 유한십진수로 표현합니다. 분모를 10의 거듭제곱으로 변경합니다.그럼 a의 값은 얼마일까요? 2²이므로 4가 됩니다. b의 값은 24, c의 값은 0.24가 됩니다. 하나하나 찾아보시나요? 이것은 당신이 답을 찾고 있는 질문입니다.'다음은 분수 7/40을 유한소수로 표현하는 과정입니다. 우리 교과서인 OT의 특징에 대해 들어보셨을 텐데요, 각 대표적인 예에는 두 가지 예가 있습니다. 참고하셔서 문제를 해결해 보시면 됩니다. 나는 그것을 할 거 야.'a의 값은 5²입니다. 얼마인가요? 25가 됩니다. b의 값이 175라면 c의 값은 얼마입니까? 0.175. 이렇게 a, b, c에 대한 답을 찾으면 됩니다.뉴런타임은 오늘 처음이었습니다. 유리수의 십진수 표현에 대해 배웠습니다. 결국 분자를 분모로 나누어 분수를 소수로 표현했습니다. 두 가지가 나왔습니다. 첫 번째는 끝이 있는 10진수입니다. 두 번째는 끝없이 계속됩니다.소수점 이하에서 일정한 구간이 반복되는 첫 번째 소절을 원형소절이라고 하였으며, 이를 표현하기 위해 양끝 위에 점을 찍었습니다. 다음 시간에 공부할 내용은 유리수, 순환소수, 유리수의 소수 표현입니다.\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "with open('final_result.txt', 'w') as f:\n",
    "    f.write('\\n'.join(textwrap.wrap(final_result, width=80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhs_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
