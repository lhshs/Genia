{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  C:\\Users\\hsl /.deepface created\n",
      "Directory  C:\\Users\\hsl /.deepface/weights created\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n",
      "Displayed the frame\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'face_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hsl\\Desktop\\Github\\YouTube_non_language\\lhshs\\sent_test.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m mp_drawing\u001b[39m.\u001b[39mdraw_landmarks(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     image\u001b[39m=\u001b[39mframe, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     landmark_list\u001b[39m=\u001b[39mface_landmarks, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     connection_drawing_spec\u001b[39m=\u001b[39mdrawing_spec\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# Extract the face from the frame\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# ...\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Analyze the emotions of the face using DeepFace\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# This is a placeholder, replace with your actual sentiment analysis code\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m emotion_analysis_results \u001b[39m=\u001b[39m DeepFace\u001b[39m.\u001b[39manalyze(face_image, actions\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Save the emotion analysis results in the dictionary\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsl/Desktop/Github/YouTube_non_language/lhshs/sent_test.ipynb#W0sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m emotion_results[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mframe_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m emotion_analysis_results[\u001b[39m'\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'face_image' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import json\n",
    "from deepface import DeepFace\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "# Initialize MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "# Initialize MediaPipe DrawingSpec\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('./sample/01강유리수의소수표현(1)_EBS중학뉴런수학2(상) (1).mp4')\n",
    "\n",
    "# Initialize dictionary to store emotion analysis results\n",
    "emotion_results = {}\n",
    "\n",
    "# Loop through each frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # print('Read a new frame')\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to detect the face and its landmarks\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    # print('Processed the frame')\n",
    "\n",
    "    # Draw the face landmarks on the frame\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame, \n",
    "                landmark_list=face_landmarks, \n",
    "                connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=drawing_spec, \n",
    "                connection_drawing_spec=drawing_spec\n",
    "                )\n",
    "            # Extract the face from the frame\n",
    "            # ...\n",
    "\n",
    "            # Analyze the emotions of the face using DeepFace\n",
    "            # This is a placeholder, replace with your actual sentiment analysis code\n",
    "            emotion_analysis_results = DeepFace.analyze(face_image, actions=['emotion'])\n",
    "\n",
    "            # Save the emotion analysis results in the dictionary\n",
    "            emotion_results[f'frame_{i}'] = emotion_analysis_results['emotion']\n",
    "            \n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    print('Displayed the frame')\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the emotion analysis results as a JSON file\n",
    "with open('emotion_results.json', 'w') as f:\n",
    "    json.dump(emotion_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "# Assume face_image is an image of a face\n",
    "emotion_analysis_results = DeepFace.analyze(face_image, actions=['emotion'])\n",
    "\n",
    "print(emotion_analysis_results['dominant_emotion'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhs_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
