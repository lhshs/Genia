{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a73e53a-645b-4a5d-8aeb-b90c8ef2d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/hslio/Desktop/Github/Genia/Study/2309_SPARK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.environ['JAVA_HOME'] = '/usr'\n",
    "os.environ['SPARK_HOME'] = '/mnt/c/Users/hslio/spark-3.3.3-bin-hadoop3' # SPARK_HOME ê²½ë¡œ\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "spark = SparkSession.builder.appName('pyspark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3034d40-bf93-42f9-ad43-ad3d6018cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame([[\"Alex\", 20],\\\n",
    "                             [\"Bob\", 24],\\\n",
    "                             [\"Cathy\", 22],\\\n",
    "                             [\"Doge\", 22]],\\\n",
    "                             [\"name\", \"age\"])\n",
    "\n",
    "sdf.show()\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2dd435-0613-4b23-8c51-1c2619122943",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2863585-f55d-422f-8a45-9cbe6d1078d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/08 17:31:38 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "Coefficients: (692,[272,300,323,350,351,378,379,405,406,407,428,433,434,435,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.520689871384157e-05,-8.11577314684704e-05,3.814692771846389e-05,0.0003776490540424341,0.0003405148366194407,0.0005514455157343111,0.00040853861160969167,0.00041974673327494573,0.0008119171358670032,0.0005027708372668752,-2.392926040660149e-05,0.0005745048020902299,0.000903754642680371,7.818229700243959e-05,-2.17875519529124e-05,-3.402165821789581e-05,0.0004966517360637634,0.0008190557828370371,-8.017982139522661e-05,-2.743169403783574e-05,0.00048108322262389896,0.00048408017626778744,-8.926472920010679e-06,-0.0003414881233042728,-8.950592574121448e-05,0.0004864546911689218,-8.478698005186158e-05,-0.0004234783215831764,-7.296535777631296e-05])\n",
      "Intercept: -0.5991460286401442\n",
      "ðŸ’•Multinomial coefficients: 2 X 692 CSRMatrix\n",
      "(0,272) 0.0001\n",
      "(0,300) 0.0001\n",
      "(0,350) -0.0002\n",
      "(0,351) -0.0001\n",
      "(0,378) -0.0003\n",
      "(0,379) -0.0002\n",
      "(0,405) -0.0002\n",
      "(0,406) -0.0004\n",
      "(0,407) -0.0002\n",
      "(0,433) -0.0003\n",
      "(0,434) -0.0005\n",
      "(0,435) -0.0001\n",
      "(0,456) 0.0\n",
      "(0,461) -0.0002\n",
      "(0,462) -0.0004\n",
      "(0,483) 0.0001\n",
      "..\n",
      "..\n",
      "â­Multinomial intercepts: [0.2750587585718083,-0.2750587585718083]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"/mnt/wslg/distro/usr/local/lib/python3.10/dist-packages/pyspark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"ðŸ’•Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"â­Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966530d1-db91-44f2-9ed6-9da7745b4e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.6833149135741672\n",
      "0.6661906127558116\n",
      "0.6207433672479604\n",
      "0.613154125312387\n",
      "0.6059149689952394\n",
      "0.5923656241678249\n",
      "0.5898233082838019\n",
      "0.5868012627420284\n",
      "0.5844432058719141\n",
      "0.5830790068041745\n",
      "0.5807015754032354\n",
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_cd7482b76a20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff3894-7459-43fa-973f-4d119a13b96d",
   "metadata": {},
   "source": [
    "### Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aff735f-9588-439f-a5e4-d84bf4dce741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|    items|freq|\n",
      "+---------+----+\n",
      "|      [1]|   3|\n",
      "|      [2]|   3|\n",
      "|   [2, 1]|   3|\n",
      "|      [5]|   2|\n",
      "|   [5, 2]|   2|\n",
      "|[5, 2, 1]|   2|\n",
      "|   [5, 1]|   2|\n",
      "+---------+----+\n",
      "\n",
      "+----------+----------+------------------+----+------------------+\n",
      "|antecedent|consequent|        confidence|lift|           support|\n",
      "+----------+----------+------------------+----+------------------+\n",
      "|       [5]|       [2]|               1.0| 1.0|0.6666666666666666|\n",
      "|       [5]|       [1]|               1.0| 1.0|0.6666666666666666|\n",
      "|    [5, 2]|       [1]|               1.0| 1.0|0.6666666666666666|\n",
      "|    [2, 1]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
      "|       [1]|       [2]|               1.0| 1.0|               1.0|\n",
      "|       [1]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
      "|       [2]|       [1]|               1.0| 1.0|               1.0|\n",
      "|       [2]|       [5]|0.6666666666666666| 1.0|0.6666666666666666|\n",
      "|    [5, 1]|       [2]|               1.0| 1.0|0.6666666666666666|\n",
      "+----------+----------+------------------+----+------------------+\n",
      "\n",
      "+---+------------+----------+\n",
      "| id|       items|prediction|\n",
      "+---+------------+----------+\n",
      "|  0|   [1, 2, 5]|        []|\n",
      "|  1|[1, 2, 3, 5]|        []|\n",
      "|  2|      [1, 2]|       [5]|\n",
      "+---+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, [1, 2, 5]),\n",
    "    (1, [1, 2, 3, 5]),\n",
    "    (2, [1, 2])\n",
    "], [\"id\", \"items\"])\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
    "model = fpGrowth.fit(df)\n",
    "\n",
    "# Display frequent itemsets.\n",
    "model.freqItemsets.show()\n",
    "\n",
    "# Display generated association rules.\n",
    "model.associationRules.show()\n",
    "\n",
    "# transform examines the input items against all the association rules and summarize the\n",
    "# consequents as prediction\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3957aa3-bd5e-4f5c-8638-41d1d097b8bb",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2042dd04-f142-4205-b740-2722c0084346",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans, KMeansModel\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load and parse the data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/mllib/kmeans_data.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m parsedData \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: array([\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)]))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Build the model (cluster the data)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"data/mllib/kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b70d1d-527a-497f-b9ba-b3da0615d8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
