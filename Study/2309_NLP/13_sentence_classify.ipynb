{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down Stream Task\n",
    "\n",
    "### 문서 분류\n",
    "\n",
    "* 데이터 : 네이버 평점 데이터 (train)\n",
    "* 분석 목적 : 네이버 평점에 대한 긍정과 부정 (이진 분류)\n",
    "* 분석 성능 기준 : 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식 패키지\n",
    "import re\n",
    "\n",
    "# 토크나이저 패키지 \n",
    "from nltk.tokenize import word_tokenize\n",
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# 벡터화 패키지\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 분류 모델 패키지\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 분류 모델 평가\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 데이터 핸들링 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 기타\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149995 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996   8549745                                      평점이 너무 낮아서...      1\n",
       "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[149995 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/ratings_train.csv')\n",
    "train_df.dropna(inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df['document'][:100000], train_df['label'][:100000]\n",
    "X_test, y_test = train_df['document'][100000:], train_df['label'][100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000,), (49995,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 토크나이저 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTokenizers :\n",
    "    def __init__(self) -> None :\n",
    "        self.okt = Twitter()\n",
    "        self.kkma = Kkma()\n",
    "        self.bpe_tokenier_pretrained = Tokenizer.from_file('./tokenizer_data/bpe_tokenizer.json')\n",
    "\n",
    "    @staticmethod\n",
    "    def whitespaceTokenizer(data : str) -> list :\n",
    "        token_rs = data.split(' ')\n",
    "        return token_rs\n",
    "    \n",
    "    @staticmethod\n",
    "    def regexsplitToken(data : str, pat : str = '[\\,\\.!?\\n]') -> list :\n",
    "        re_rs = re.split(pat, data, maxsplit=0)\n",
    "        token_rs = [rs_unit.strip() for rs_unit in re_rs if len(rs_unit.strip()) > 1]\n",
    "        return token_rs\n",
    "    \n",
    "    @staticmethod\n",
    "    def regexselectToken(data : str, pat : str = '[\\w]+') -> list :\n",
    "        token_rs = RegexpTokenizer(pat).tokenize(data)\n",
    "        return token_rs\n",
    "    \n",
    "    def BPETokenizer(self, data : str) -> list :\n",
    "        token_rs = self.bpe_tokenier_pretrained.encode(data).tokens\n",
    "        return token_rs\n",
    "    \n",
    "    # 한글, 영어 같이 사용\n",
    "    def tokenizingKorEng(self, data : str) -> list :\n",
    "        kor_re = re.findall('[ㄱ-ㅎㅏ-ㅣ가-힣]+', data)\n",
    "        kor_str = ' '.join(kor_re)\n",
    "\n",
    "        eng_re = re.findall('[a-zA-Z]+', data)\n",
    "        eng_str = ' '.join(eng_re)\n",
    "\n",
    "        kor_rs = self.kot.morphs(kor_str)\n",
    "        eng_rs = word_tokenize(eng_str)\n",
    "\n",
    "        token_rs = kor_rs + eng_rs\n",
    "\n",
    "        return token_rs\n",
    "    \n",
    "    # 명사만 뽑는 tokenizer\n",
    "    def konlpyNounsTokenizer(self, data : str) -> list : \n",
    "        token_rs = self.kkma.nouns(data)\n",
    "        return token_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hslio\\anaconda3\\envs\\lhs_3.10\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.UserTokenizers at 0x172e3d68610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut_cls = UserTokenizers()\n",
    "ut_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['걸음마',\n",
       " '3',\n",
       " '3세',\n",
       " '세',\n",
       " '초등학교',\n",
       " '1',\n",
       " '1학년생인',\n",
       " '학년',\n",
       " '생인',\n",
       " '8',\n",
       " '8살용영화',\n",
       " '살',\n",
       " '용',\n",
       " '영화',\n",
       " '별',\n",
       " '별반개',\n",
       " '반개']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut_cls.konlpyNounsTokenizer(X_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['막</w>',\n",
       " '걸',\n",
       " '음',\n",
       " '마</w>',\n",
       " '<unk>',\n",
       " '3',\n",
       " '세',\n",
       " '부터</w>',\n",
       " '초등학교</w>',\n",
       " '1',\n",
       " '학년',\n",
       " '생',\n",
       " '인</w>',\n",
       " '8',\n",
       " '살',\n",
       " '용',\n",
       " '영화</w>',\n",
       " '.</w>',\n",
       " 'ㅋㅋㅋ</w>',\n",
       " '.</w>',\n",
       " '.</w>',\n",
       " '.</w>',\n",
       " '별',\n",
       " '반개도</w>',\n",
       " '아까움</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut_cls.BPETokenizer(X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의', '가', '은', '는', '이', '을', '를', '으로', '에', '과']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(tokenizer=ut_cls.BPETokenizer, stop_words=stopwords)#, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hslio\\anaconda3\\envs\\lhs_3.10\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\hslio\\anaconda3\\envs\\lhs_3.10\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['가</w>', '과</w>', '는</w>', '를</w>', '에</w>', '으로</w>', '은</w>', '을</w>', '의</w>', '이</w>'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(stop_words=[&#x27;의&#x27;, &#x27;가&#x27;, &#x27;은&#x27;, &#x27;는&#x27;, &#x27;이&#x27;, &#x27;을&#x27;, &#x27;를&#x27;, &#x27;으로&#x27;, &#x27;에&#x27;, &#x27;과&#x27;],\n",
       "                tokenizer=&lt;bound method UserTokenizers.BPETokenizer of &lt;__main__.UserTokenizers object at 0x00000172E3D68610&gt;&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=[&#x27;의&#x27;, &#x27;가&#x27;, &#x27;은&#x27;, &#x27;는&#x27;, &#x27;이&#x27;, &#x27;을&#x27;, &#x27;를&#x27;, &#x27;으로&#x27;, &#x27;에&#x27;, &#x27;과&#x27;],\n",
       "                tokenizer=&lt;bound method UserTokenizers.BPETokenizer of &lt;__main__.UserTokenizers object at 0x00000172E3D68610&gt;&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(stop_words=['의', '가', '은', '는', '이', '을', '를', '으로', '에', '과'],\n",
       "                tokenizer=<bound method UserTokenizers.BPETokenizer of <__main__.UserTokenizers object at 0x00000172E3D68610>>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!</w>', '\"</w>', '%</w>', ..., '힘들었다</w>', '힘을</w>', '힘이</w>'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformVect : \n",
    "    def __init__(self, vec_model) -> None :\n",
    "        # 서비스로 개발한다면, 저장된 vec_model을 로딩하는 부분으로 변경 \n",
    "        self.vec_model = vec_model\n",
    "    \n",
    "    def transVect_run(self, chunk_size : int=500, data : pd.DataFrame=None) -> np.array :\n",
    "\n",
    "        data_len = len(data)\n",
    "\n",
    "        for st_idx in tqdm(range(0, data_len, chunk_size)):\n",
    "            tmp_data = data[st_idx : st_idx + chunk_size]\n",
    "\n",
    "            if st_idx == 0 :\n",
    "                vec_arr = self.vec_model.transform(tmp_data).toarray()\n",
    "            \n",
    "            else : \n",
    "                tmp_data_arr = self.vec_model.transform(tmp_data).toarray()\n",
    "                vec_arr = np.concatenate([vec_arr, tmp_data_arr], 0)\n",
    "        \n",
    "        return vec_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv_cls = TransformVect(tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [22:16<00:00,  1.34s/it]\n",
      "100%|██████████| 500/500 [03:51<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train = X_train[:500], y_train[:500]\n",
    "# X_test, y_test = X_test[:100], y_test[:100]\n",
    "\n",
    "# x_train = tfidf_vect.transform(X_train).toarray()\n",
    "# x_test = tfidf_vect.transform(X_test).toarray()\n",
    "\n",
    "x_train = tfv_cls.transVect_run(chunk_size=100, data=X_train)\n",
    "x_test = tfv_cls.transVect_run(chunk_size=100, data=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 9859), (49995, 9859))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "## Save pickle\n",
    "# with open(\"x_train.pickle\",\"wb\") as fw:\n",
    "#    pickle.dump(x_train, fw)\n",
    "\n",
    "# with open(\"x_test.pickle\",\"wb\") as fw:\n",
    "#   pickle.dump(x_test, fw)\n",
    " \n",
    "## Load pickle\n",
    "# with open(\"x_test.pickle\",\"rb\") as fr:\n",
    "#    data = pickle.load(fr)\n",
    "# print(data)\n",
    "#['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고정된 모델 결과\n",
    "random_seed_num = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.95      0.69     50115\n",
      "           1       0.80      0.19      0.31     49885\n",
      "\n",
      "    accuracy                           0.57    100000\n",
      "   macro avg       0.67      0.57      0.50    100000\n",
      "weighted avg       0.67      0.57      0.50    100000\n",
      "\n",
      "💕\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.95      0.69     25055\n",
      "           1       0.78      0.19      0.30     24940\n",
      "\n",
      "    accuracy                           0.57     49995\n",
      "   macro avg       0.66      0.57      0.50     49995\n",
      "weighted avg       0.66      0.57      0.50     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "clf_decision = DecisionTreeClassifier(random_state=random_seed_num, max_depth=5)\n",
    "clf_decision.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_decision.predict(x_train)\n",
    "y_test_pred = clf_decision.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('💕')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.88      0.76     50115\n",
      "           1       0.82      0.55      0.66     49885\n",
      "\n",
      "    accuracy                           0.72    100000\n",
      "   macro avg       0.74      0.72      0.71    100000\n",
      "weighted avg       0.74      0.72      0.71    100000\n",
      "\n",
      "💕\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.87      0.75     25055\n",
      "           1       0.80      0.55      0.65     24940\n",
      "\n",
      "    accuracy                           0.71     49995\n",
      "   macro avg       0.73      0.71      0.70     49995\n",
      "weighted avg       0.73      0.71      0.70     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "rfc_decision = RandomForestClassifier(random_state=random_seed_num, max_depth=5)\n",
    "rfc_decision.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = rfc_decision.predict(x_train)\n",
    "y_test_pred = rfc_decision.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('💕')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     50115\n",
      "           1       1.00      1.00      1.00     49885\n",
      "\n",
      "    accuracy                           1.00    100000\n",
      "   macro avg       1.00      1.00      1.00    100000\n",
      "weighted avg       1.00      1.00      1.00    100000\n",
      "\n",
      "💕\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.76     25055\n",
      "           1       0.76      0.77      0.76     24940\n",
      "\n",
      "    accuracy                           0.76     49995\n",
      "   macro avg       0.76      0.76      0.76     49995\n",
      "weighted avg       0.76      0.76      0.76     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-neighbors\n",
    "\n",
    "clf_kneighbors = KNeighborsClassifier(n_neighbors=3, weights='distance', leaf_size=50)\n",
    "clf_kneighbors.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_kneighbors.predict(x_train)\n",
    "y_test_pred = clf_kneighbors.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('💕')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88     50115\n",
      "           1       0.88      0.87      0.88     49885\n",
      "\n",
      "    accuracy                           0.88    100000\n",
      "   macro avg       0.88      0.88      0.88    100000\n",
      "weighted avg       0.88      0.88      0.88    100000\n",
      "\n",
      "💕\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85     25055\n",
      "           1       0.85      0.84      0.85     24940\n",
      "\n",
      "    accuracy                           0.85     49995\n",
      "   macro avg       0.85      0.85      0.85     49995\n",
      "weighted avg       0.85      0.85      0.85     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic \n",
    "\n",
    "clf_logistic = LogisticRegression(max_iter=3000, random_state=random_seed_num)\n",
    "clf_logistic.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_logistic.predict(x_train)\n",
    "y_test_pred = clf_logistic.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('💕')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hslio\\anaconda3\\envs\\lhs_3.10\\lib\\site-packages\\xgboost\\data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.86      0.79     50115\n",
      "           1       0.83      0.68      0.75     49885\n",
      "\n",
      "    accuracy                           0.77    100000\n",
      "   macro avg       0.78      0.77      0.77    100000\n",
      "weighted avg       0.78      0.77      0.77    100000\n",
      "\n",
      "💕\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.84      0.77     25055\n",
      "           1       0.80      0.66      0.73     24940\n",
      "\n",
      "    accuracy                           0.75     49995\n",
      "   macro avg       0.76      0.75      0.75     49995\n",
      "weighted avg       0.76      0.75      0.75     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgb\n",
    "\n",
    "clf_xgb = XGBClassifier(random_state=random_seed_num, max_depth=5)\n",
    "clf_xgb.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_xgb.predict(x_train)\n",
    "y_test_pred = clf_xgb.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('💕')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     50115\n",
      "           1       0.99      0.99      0.99     49885\n",
      "\n",
      "    accuracy                           0.99    100000\n",
      "   macro avg       0.99      0.99      0.99    100000\n",
      "weighted avg       0.99      0.99      0.99    100000\n",
      "\n",
      "💕\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83     25055\n",
      "           1       0.84      0.83      0.83     24940\n",
      "\n",
      "    accuracy                           0.83     49995\n",
      "   macro avg       0.83      0.83      0.83     49995\n",
      "weighted avg       0.83      0.83      0.83     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "\n",
    "# voting 방식\n",
    "## soft : 각 모델의 확률값을 사용해서 결과 도출 \n",
    "## hard : 각 모델의 최종 레이블에서 다수로 분류 된 값으로 결과 도출 \n",
    "\n",
    "clf_ensemble = VotingClassifier(estimators=[('logistic', clf_logistic), \n",
    "                                            ('KNN', clf_kneighbors), \n",
    "                                            ('XGB', clf_xgb)], \n",
    "                                            voting='soft')\n",
    "clf_ensemble.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_ensemble.predict(x_train)\n",
    "y_test_pred = clf_ensemble.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('💕')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리 에러가 나는 경우의 대처 방법\n",
    "\n",
    "1. 하드웨어 변경\n",
    "    - 메모리 up\n",
    "    - 메모리가 넉넉한 서버로 이동 후 분석\n",
    "2. 코드로 제어\n",
    "    - chunk_size : 분석 분할\n",
    "    - 모델 제어 : word2vec / countVect / tfidfVect(통계 기반으로 메모리 사용이 많을 수 있음)\n",
    "    <br>=> vocab 사이즈 제어\n",
    "    <br>=> 피처 추출 : 전체 토큰화된 텍스트 데이터 -> 명사만 분석 \n",
    "\n",
    "- 에러가 나는 케이스 \n",
    "    - numpy : index 값이 특정값 이상(int64)인 경우 sorting 등에서 에러 발생 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhs_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
