{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down Stream Task\n",
    "\n",
    "### ë¬¸ì„œ ë¶„ë¥˜\n",
    "\n",
    "* ë°ì´í„° : ë„¤ì´ë²„ í‰ì  ë°ì´í„° (train)\n",
    "* ë¶„ì„ ëª©ì  : ë„¤ì´ë²„ í‰ì ì— ëŒ€í•œ ê¸ì •ê³¼ ë¶€ì • (ì´ì§„ ë¶„ë¥˜)\n",
    "* ë¶„ì„ ì„±ëŠ¥ ê¸°ì¤€ : ì •í™•ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ê·œ í‘œí˜„ì‹ íŒ¨í‚¤ì§€\n",
    "import re\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € íŒ¨í‚¤ì§€ \n",
    "from nltk.tokenize import word_tokenize\n",
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# ë²¡í„°í™” íŒ¨í‚¤ì§€\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ë¶„ë¥˜ ëª¨ë¸ íŒ¨í‚¤ì§€\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ë¶„ë¥˜ ëª¨ë¸ í‰ê°€\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ë°ì´í„° í•¸ë“¤ë§ \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ê¸°íƒ€\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>ì¸ê°„ì´ ë¬¸ì œì§€.. ì†ŒëŠ” ë­”ì£„ì¸ê°€..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>í‰ì ì´ ë„ˆë¬´ ë‚®ì•„ì„œ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>ì´ê²Œ ë­ìš”? í•œêµ­ì¸ì€ ê±°ë“¤ë¨¹ê±°ë¦¬ê³  í•„ë¦¬í•€ í˜¼í˜ˆì€ ì°©í•˜ë‹¤?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>ì²­ì¶˜ ì˜í™”ì˜ ìµœê³ ë´‰.ë°©í™©ê³¼ ìš°ìš¸í–ˆë˜ ë‚ ë“¤ì˜ ìí™”ìƒ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>í•œêµ­ ì˜í™” ìµœì´ˆë¡œ ìˆ˜ê°„í•˜ëŠ” ë‚´ìš©ì´ ë‹´ê¸´ ì˜í™”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149995 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬      0\n",
       "1        3819312                  í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜      1\n",
       "2       10265843                                  ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤      0\n",
       "3        9045019                      êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •      0\n",
       "4        6483659  ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                ì¸ê°„ì´ ë¬¸ì œì§€.. ì†ŒëŠ” ë­”ì£„ì¸ê°€..      0\n",
       "149996   8549745                                      í‰ì ì´ ë„ˆë¬´ ë‚®ì•„ì„œ...      1\n",
       "149997   9311800                    ì´ê²Œ ë­ìš”? í•œêµ­ì¸ì€ ê±°ë“¤ë¨¹ê±°ë¦¬ê³  í•„ë¦¬í•€ í˜¼í˜ˆì€ ì°©í•˜ë‹¤?      0\n",
       "149998   2376369                        ì²­ì¶˜ ì˜í™”ì˜ ìµœê³ ë´‰.ë°©í™©ê³¼ ìš°ìš¸í–ˆë˜ ë‚ ë“¤ì˜ ìí™”ìƒ      1\n",
       "149999   9619869                           í•œêµ­ ì˜í™” ìµœì´ˆë¡œ ìˆ˜ê°„í•˜ëŠ” ë‚´ìš©ì´ ë‹´ê¸´ ì˜í™”      0\n",
       "\n",
       "[149995 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/ratings_train.csv')\n",
    "train_df.dropna(inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df['document'][:100000], train_df['label'][:100000]\n",
    "X_test, y_test = train_df['document'][100000:], train_df['label'][100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000,), (49995,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‚¬ìš©ì í† í¬ë‚˜ì´ì € ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTokenizers :\n",
    "    def __init__(self) -> None :\n",
    "        self.okt = Twitter()\n",
    "        self.kkma = Kkma()\n",
    "        self.bpe_tokenier_pretrained = Tokenizer.from_file('./tokenizer_data/bpe_tokenizer.json')\n",
    "\n",
    "    @staticmethod\n",
    "    def whitespaceTokenizer(data : str) -> list :\n",
    "        token_rs = data.split(' ')\n",
    "        return token_rs\n",
    "    \n",
    "    @staticmethod\n",
    "    def regexsplitToken(data : str, pat : str = '[\\,\\.!?\\n]') -> list :\n",
    "        re_rs = re.split(pat, data, maxsplit=0)\n",
    "        token_rs = [rs_unit.strip() for rs_unit in re_rs if len(rs_unit.strip()) > 1]\n",
    "        return token_rs\n",
    "    \n",
    "    @staticmethod\n",
    "    def regexselectToken(data : str, pat : str = '[\\w]+') -> list :\n",
    "        token_rs = RegexpTokenizer(pat).tokenize(data)\n",
    "        return token_rs\n",
    "    \n",
    "    def BPETokenizer(self, data : str) -> list :\n",
    "        token_rs = self.bpe_tokenier_pretrained.encode(data).tokens\n",
    "        return token_rs\n",
    "    \n",
    "    # í•œê¸€, ì˜ì–´ ê°™ì´ ì‚¬ìš©\n",
    "    def tokenizingKorEng(self, data : str) -> list :\n",
    "        kor_re = re.findall('[ã„±-ã…ã…-ã…£ê°€-í£]+', data)\n",
    "        kor_str = ' '.join(kor_re)\n",
    "\n",
    "        eng_re = re.findall('[a-zA-Z]+', data)\n",
    "        eng_str = ' '.join(eng_re)\n",
    "\n",
    "        kor_rs = self.kot.morphs(kor_str)\n",
    "        eng_rs = word_tokenize(eng_str)\n",
    "\n",
    "        token_rs = kor_rs + eng_rs\n",
    "\n",
    "        return token_rs\n",
    "    \n",
    "    # ëª…ì‚¬ë§Œ ë½‘ëŠ” tokenizer\n",
    "    def konlpyNounsTokenizer(self, data : str) -> list : \n",
    "        token_rs = self.kkma.nouns(data)\n",
    "        return token_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hslio\\anaconda3\\envs\\lhs_3.10\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.UserTokenizers at 0x172e3d68610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut_cls = UserTokenizers()\n",
    "ut_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ê±¸ìŒë§ˆ',\n",
       " '3',\n",
       " '3ì„¸',\n",
       " 'ì„¸',\n",
       " 'ì´ˆë“±í•™êµ',\n",
       " '1',\n",
       " '1í•™ë…„ìƒì¸',\n",
       " 'í•™ë…„',\n",
       " 'ìƒì¸',\n",
       " '8',\n",
       " '8ì‚´ìš©ì˜í™”',\n",
       " 'ì‚´',\n",
       " 'ìš©',\n",
       " 'ì˜í™”',\n",
       " 'ë³„',\n",
       " 'ë³„ë°˜ê°œ',\n",
       " 'ë°˜ê°œ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut_cls.konlpyNounsTokenizer(X_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ë§‰</w>',\n",
       " 'ê±¸',\n",
       " 'ìŒ',\n",
       " 'ë§ˆ</w>',\n",
       " '<unk>',\n",
       " '3',\n",
       " 'ì„¸',\n",
       " 'ë¶€í„°</w>',\n",
       " 'ì´ˆë“±í•™êµ</w>',\n",
       " '1',\n",
       " 'í•™ë…„',\n",
       " 'ìƒ',\n",
       " 'ì¸</w>',\n",
       " '8',\n",
       " 'ì‚´',\n",
       " 'ìš©',\n",
       " 'ì˜í™”</w>',\n",
       " '.</w>',\n",
       " 'ã…‹ã…‹ã…‹</w>',\n",
       " '.</w>',\n",
       " '.</w>',\n",
       " '.</w>',\n",
       " 'ë³„',\n",
       " 'ë°˜ê°œë„</w>',\n",
       " 'ì•„ê¹Œì›€</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut_cls.BPETokenizer(X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¶ˆìš©ì–´ ì ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ì´', 'ì„', 'ë¥¼', 'ìœ¼ë¡œ', 'ì—', 'ê³¼']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë²¡í„°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(tokenizer=ut_cls.BPETokenizer, stop_words=stopwords)#, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hslio\\anaconda3\\envs\\lhs_3.10\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\hslio\\anaconda3\\envs\\lhs_3.10\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ê°€</w>', 'ê³¼</w>', 'ëŠ”</w>', 'ë¥¼</w>', 'ì—</w>', 'ìœ¼ë¡œ</w>', 'ì€</w>', 'ì„</w>', 'ì˜</w>', 'ì´</w>'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(stop_words=[&#x27;ì˜&#x27;, &#x27;ê°€&#x27;, &#x27;ì€&#x27;, &#x27;ëŠ”&#x27;, &#x27;ì´&#x27;, &#x27;ì„&#x27;, &#x27;ë¥¼&#x27;, &#x27;ìœ¼ë¡œ&#x27;, &#x27;ì—&#x27;, &#x27;ê³¼&#x27;],\n",
       "                tokenizer=&lt;bound method UserTokenizers.BPETokenizer of &lt;__main__.UserTokenizers object at 0x00000172E3D68610&gt;&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=[&#x27;ì˜&#x27;, &#x27;ê°€&#x27;, &#x27;ì€&#x27;, &#x27;ëŠ”&#x27;, &#x27;ì´&#x27;, &#x27;ì„&#x27;, &#x27;ë¥¼&#x27;, &#x27;ìœ¼ë¡œ&#x27;, &#x27;ì—&#x27;, &#x27;ê³¼&#x27;],\n",
       "                tokenizer=&lt;bound method UserTokenizers.BPETokenizer of &lt;__main__.UserTokenizers object at 0x00000172E3D68610&gt;&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(stop_words=['ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ì´', 'ì„', 'ë¥¼', 'ìœ¼ë¡œ', 'ì—', 'ê³¼'],\n",
       "                tokenizer=<bound method UserTokenizers.BPETokenizer of <__main__.UserTokenizers object at 0x00000172E3D68610>>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!</w>', '\"</w>', '%</w>', ..., 'í˜ë“¤ì—ˆë‹¤</w>', 'í˜ì„</w>', 'í˜ì´</w>'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformVect : \n",
    "    def __init__(self, vec_model) -> None :\n",
    "        # ì„œë¹„ìŠ¤ë¡œ ê°œë°œí•œë‹¤ë©´, ì €ì¥ëœ vec_modelì„ ë¡œë”©í•˜ëŠ” ë¶€ë¶„ìœ¼ë¡œ ë³€ê²½ \n",
    "        self.vec_model = vec_model\n",
    "    \n",
    "    def transVect_run(self, chunk_size : int=500, data : pd.DataFrame=None) -> np.array :\n",
    "\n",
    "        data_len = len(data)\n",
    "\n",
    "        for st_idx in tqdm(range(0, data_len, chunk_size)):\n",
    "            tmp_data = data[st_idx : st_idx + chunk_size]\n",
    "\n",
    "            if st_idx == 0 :\n",
    "                vec_arr = self.vec_model.transform(tmp_data).toarray()\n",
    "            \n",
    "            else : \n",
    "                tmp_data_arr = self.vec_model.transform(tmp_data).toarray()\n",
    "                vec_arr = np.concatenate([vec_arr, tmp_data_arr], 0)\n",
    "        \n",
    "        return vec_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv_cls = TransformVect(tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [22:16<00:00,  1.34s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [03:51<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train = X_train[:500], y_train[:500]\n",
    "# X_test, y_test = X_test[:100], y_test[:100]\n",
    "\n",
    "# x_train = tfidf_vect.transform(X_train).toarray()\n",
    "# x_test = tfidf_vect.transform(X_test).toarray()\n",
    "\n",
    "x_train = tfv_cls.transVect_run(chunk_size=100, data=X_train)\n",
    "x_test = tfv_cls.transVect_run(chunk_size=100, data=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 9859), (49995, 9859))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "## Save pickle\n",
    "# with open(\"x_train.pickle\",\"wb\") as fw:\n",
    "#    pickle.dump(x_train, fw)\n",
    "\n",
    "# with open(\"x_test.pickle\",\"wb\") as fw:\n",
    "#   pickle.dump(x_test, fw)\n",
    " \n",
    "## Load pickle\n",
    "# with open(\"x_test.pickle\",\"rb\") as fr:\n",
    "#    data = pickle.load(fr)\n",
    "# print(data)\n",
    "#['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¶„ë¥˜ ëª¨ë¸ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ì •ëœ ëª¨ë¸ ê²°ê³¼\n",
    "random_seed_num = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.95      0.69     50115\n",
      "           1       0.80      0.19      0.31     49885\n",
      "\n",
      "    accuracy                           0.57    100000\n",
      "   macro avg       0.67      0.57      0.50    100000\n",
      "weighted avg       0.67      0.57      0.50    100000\n",
      "\n",
      "ğŸ’•\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.95      0.69     25055\n",
      "           1       0.78      0.19      0.30     24940\n",
      "\n",
      "    accuracy                           0.57     49995\n",
      "   macro avg       0.66      0.57      0.50     49995\n",
      "weighted avg       0.66      0.57      0.50     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "clf_decision = DecisionTreeClassifier(random_state=random_seed_num, max_depth=5)\n",
    "clf_decision.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_decision.predict(x_train)\n",
    "y_test_pred = clf_decision.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('ğŸ’•')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.88      0.76     50115\n",
      "           1       0.82      0.55      0.66     49885\n",
      "\n",
      "    accuracy                           0.72    100000\n",
      "   macro avg       0.74      0.72      0.71    100000\n",
      "weighted avg       0.74      0.72      0.71    100000\n",
      "\n",
      "ğŸ’•\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.87      0.75     25055\n",
      "           1       0.80      0.55      0.65     24940\n",
      "\n",
      "    accuracy                           0.71     49995\n",
      "   macro avg       0.73      0.71      0.70     49995\n",
      "weighted avg       0.73      0.71      0.70     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "rfc_decision = RandomForestClassifier(random_state=random_seed_num, max_depth=5)\n",
    "rfc_decision.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = rfc_decision.predict(x_train)\n",
    "y_test_pred = rfc_decision.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('ğŸ’•')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     50115\n",
      "           1       1.00      1.00      1.00     49885\n",
      "\n",
      "    accuracy                           1.00    100000\n",
      "   macro avg       1.00      1.00      1.00    100000\n",
      "weighted avg       1.00      1.00      1.00    100000\n",
      "\n",
      "ğŸ’•\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.76     25055\n",
      "           1       0.76      0.77      0.76     24940\n",
      "\n",
      "    accuracy                           0.76     49995\n",
      "   macro avg       0.76      0.76      0.76     49995\n",
      "weighted avg       0.76      0.76      0.76     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-neighbors\n",
    "\n",
    "clf_kneighbors = KNeighborsClassifier(n_neighbors=3, weights='distance', leaf_size=50)\n",
    "clf_kneighbors.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_kneighbors.predict(x_train)\n",
    "y_test_pred = clf_kneighbors.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('ğŸ’•')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88     50115\n",
      "           1       0.88      0.87      0.88     49885\n",
      "\n",
      "    accuracy                           0.88    100000\n",
      "   macro avg       0.88      0.88      0.88    100000\n",
      "weighted avg       0.88      0.88      0.88    100000\n",
      "\n",
      "ğŸ’•\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85     25055\n",
      "           1       0.85      0.84      0.85     24940\n",
      "\n",
      "    accuracy                           0.85     49995\n",
      "   macro avg       0.85      0.85      0.85     49995\n",
      "weighted avg       0.85      0.85      0.85     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic \n",
    "\n",
    "clf_logistic = LogisticRegression(max_iter=3000, random_state=random_seed_num)\n",
    "clf_logistic.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_logistic.predict(x_train)\n",
    "y_test_pred = clf_logistic.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('ğŸ’•')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hslio\\anaconda3\\envs\\lhs_3.10\\lib\\site-packages\\xgboost\\data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.86      0.79     50115\n",
      "           1       0.83      0.68      0.75     49885\n",
      "\n",
      "    accuracy                           0.77    100000\n",
      "   macro avg       0.78      0.77      0.77    100000\n",
      "weighted avg       0.78      0.77      0.77    100000\n",
      "\n",
      "ğŸ’•\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.84      0.77     25055\n",
      "           1       0.80      0.66      0.73     24940\n",
      "\n",
      "    accuracy                           0.75     49995\n",
      "   macro avg       0.76      0.75      0.75     49995\n",
      "weighted avg       0.76      0.75      0.75     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgb\n",
    "\n",
    "clf_xgb = XGBClassifier(random_state=random_seed_num, max_depth=5)\n",
    "clf_xgb.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_xgb.predict(x_train)\n",
    "y_test_pred = clf_xgb.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('ğŸ’•')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     50115\n",
      "           1       0.99      0.99      0.99     49885\n",
      "\n",
      "    accuracy                           0.99    100000\n",
      "   macro avg       0.99      0.99      0.99    100000\n",
      "weighted avg       0.99      0.99      0.99    100000\n",
      "\n",
      "ğŸ’•\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83     25055\n",
      "           1       0.84      0.83      0.83     24940\n",
      "\n",
      "    accuracy                           0.83     49995\n",
      "   macro avg       0.83      0.83      0.83     49995\n",
      "weighted avg       0.83      0.83      0.83     49995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "\n",
    "# voting ë°©ì‹\n",
    "## soft : ê° ëª¨ë¸ì˜ í™•ë¥ ê°’ì„ ì‚¬ìš©í•´ì„œ ê²°ê³¼ ë„ì¶œ \n",
    "## hard : ê° ëª¨ë¸ì˜ ìµœì¢… ë ˆì´ë¸”ì—ì„œ ë‹¤ìˆ˜ë¡œ ë¶„ë¥˜ ëœ ê°’ìœ¼ë¡œ ê²°ê³¼ ë„ì¶œ \n",
    "\n",
    "clf_ensemble = VotingClassifier(estimators=[('logistic', clf_logistic), \n",
    "                                            ('KNN', clf_kneighbors), \n",
    "                                            ('XGB', clf_xgb)], \n",
    "                                            voting='soft')\n",
    "clf_ensemble.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf_ensemble.predict(x_train)\n",
    "y_test_pred = clf_ensemble.predict(x_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print('ğŸ’•')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë©”ëª¨ë¦¬ ì—ëŸ¬ê°€ ë‚˜ëŠ” ê²½ìš°ì˜ ëŒ€ì²˜ ë°©ë²•\n",
    "\n",
    "1. í•˜ë“œì›¨ì–´ ë³€ê²½\n",
    "    - ë©”ëª¨ë¦¬ up\n",
    "    - ë©”ëª¨ë¦¬ê°€ ë„‰ë„‰í•œ ì„œë²„ë¡œ ì´ë™ í›„ ë¶„ì„\n",
    "2. ì½”ë“œë¡œ ì œì–´\n",
    "    - chunk_size : ë¶„ì„ ë¶„í• \n",
    "    - ëª¨ë¸ ì œì–´ : word2vec / countVect / tfidfVect(í†µê³„ ê¸°ë°˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì´ ë§ì„ ìˆ˜ ìˆìŒ)\n",
    "    <br>=> vocab ì‚¬ì´ì¦ˆ ì œì–´\n",
    "    <br>=> í”¼ì²˜ ì¶”ì¶œ : ì „ì²´ í† í°í™”ëœ í…ìŠ¤íŠ¸ ë°ì´í„° -> ëª…ì‚¬ë§Œ ë¶„ì„ \n",
    "\n",
    "- ì—ëŸ¬ê°€ ë‚˜ëŠ” ì¼€ì´ìŠ¤ \n",
    "    - numpy : index ê°’ì´ íŠ¹ì •ê°’ ì´ìƒ(int64)ì¸ ê²½ìš° sorting ë“±ì—ì„œ ì—ëŸ¬ ë°œìƒ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhs_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
